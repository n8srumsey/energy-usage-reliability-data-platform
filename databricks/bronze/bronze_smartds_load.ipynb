{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d406f6fe-f364-476c-9d83-1c125329cb4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0ddc7d-34bf-4dac-9c7d-86127d8eee9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "SMART_DS_BUCKET = \"s3://oedi-data-lake\"\n",
    "SMART_DS_PREFIX = \"SMART-DS/v1.0\"\n",
    "\n",
    "YEARS = [\"2016\"]\n",
    "CITY = \"SFO\"\n",
    "SUBSTATION = \"P35U\"\n",
    "\n",
    "CATALOG = \"energy_data_platform_project\"\n",
    "SCHEMA = \"bronze\"\n",
    "VOLUME = \"smartds_raw\"\n",
    "\n",
    "BRONZE_PATH = f\"s3://energy-data-platform-project-bucket/etl/bronze/smartds_sfo_load\"\n",
    "BRONZE_TABLE = f\"{CATALOG}.{SCHEMA}.smartds_sfo_load\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "545dcd41-a73b-490a-af7e-f085b157da68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "NUM_COLS = [\n",
    "    \"total_site_electricity_kw\",\n",
    "    \"total_site_electricity_kvar\",\n",
    "    \"pf\",\n",
    "    \"heating_kw\", \"heating_kvar\",\n",
    "    \"cooling_kw\", \"cooling_kvar\",\n",
    "    \"lighting_kw\", \"lighting_kvar\",\n",
    "    \"fans_kw\", \"fans_kvar\",\n",
    "    \"pumps_kw\", \"pumps_kvar\",\n",
    "    \"water_systems_kw\", \"water_systems_kvar\",\n",
    "    \"refrigeration_kw\", \"refrigeration_kvar\",\n",
    "    \"motors_kw\", \"motors_kvar\",\n",
    "    \"plug_loads_kw\", \"plug_loads_kvar\",\n",
    "    \"clothes_dryer_kw\", \"clothes_dryer_kvar\",\n",
    "    \"clothes_washer_kw\", \"clothes_washer_kvar\",\n",
    "    \"stove_kw\", \"stove_kvar\",\n",
    "    \"dishwasher_kw\", \"dishwasher_kvar\",\n",
    "]\n",
    "\n",
    "def cast_numeric_columns(df):\n",
    "    \"\"\"\n",
    "    For each column in NUM_COLS:\n",
    "      - if present, cast to DoubleType\n",
    "      - if missing in this file, add as NULL DoubleType\n",
    "    Ensures a consistent numeric schema across all files.\n",
    "    \"\"\"\n",
    "    for c in NUM_COLS:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, F.col(c).cast(DoubleType()))\n",
    "        else:\n",
    "            df = df.withColumn(c, F.lit(None).cast(DoubleType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_single_parquet(path: str, year: str, city: str, substation_id: str):\n",
    "    \"\"\"\n",
    "    Read a single SMART-DS parquet file, normalize numeric columns,\n",
    "    and attach Bronze metadata columns.\n",
    "    \"\"\"\n",
    "    df = spark.read.parquet(path)\n",
    "    df = cast_numeric_columns(df)\n",
    "\n",
    "    df = (\n",
    "        df\n",
    "        # Bronze metadata injected here (no regex needed)\n",
    "        .withColumn(\"source_file\", F.lit(path))\n",
    "        .withColumn(\"city\", F.lit(city).cast(StringType()))\n",
    "        .withColumn(\"year\", F.lit(year).cast(StringType()))\n",
    "        .withColumn(\"substation_id\", F.lit(substation_id).cast(StringType()))\n",
    "        .withColumn(\"ingest_ts\", F.current_timestamp())\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9224437-cddd-49fe-bd48-6fc136aa2a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- 1. Enumerate only the target substation's load_data parquet files ----\n",
    "all_dfs = []\n",
    "\n",
    "for year in YEARS:\n",
    "    year_root = f\"{SMART_DS_BUCKET}/{SMART_DS_PREFIX}/{year}/{CITY}\"\n",
    "    print(f\"Listing substations under {year_root}\")\n",
    "    for sub in dbutils.fs.ls(year_root):\n",
    "        substation_id = sub.name.rstrip(\"/\")   # e.g. \"P10U\"\n",
    "\n",
    "        # Filter to one substation if TARGET_SUBSTATION is set\n",
    "        if SUBSTATION is not None and substation_id != SUBSTATION:\n",
    "            continue\n",
    "\n",
    "        load_dir = f\"{sub.path}load_data/\"\n",
    "        try:\n",
    "            files = dbutils.fs.ls(load_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {load_dir}: {e}\")\n",
    "            continue\n",
    "\n",
    "        parquet_files = [f for f in files if f.name.endswith(\".parquet\")]\n",
    "        print(f\"  {year} {substation_id}: {len(parquet_files)} parquet files\")\n",
    "\n",
    "        for f in parquet_files:\n",
    "            df_file = load_single_parquet(f.path, year, CITY, substation_id)\n",
    "            all_dfs.append(df_file)\n",
    "\n",
    "print(f\"Total files loaded: {len(all_dfs)}\")\n",
    "\n",
    "if not all_dfs:\n",
    "    raise Exception(\n",
    "        f\"No SMART-DS parquet files found for years={YEARS}, city={CITY}, substation={SUBSTATION}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb17ff2a-6670-4b87-aded-dc556973133a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- 2. Union all per-file DataFrames into one Bronze DataFrame ----\n",
    "bronze_df = reduce(\n",
    "    lambda a, b: a.unionByName(b, allowMissingColumns=True),\n",
    "    all_dfs\n",
    ")\n",
    "\n",
    "print(\"Bronze row count:\", bronze_df.count())\n",
    "display(bronze_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e50a4d-dff9-4aff-b9e5-66c985e42226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---- 3. Write Bronze Delta & register table ----\n",
    "(\n",
    "    bronze_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"year\")\n",
    "    .saveAsTable(BRONZE_TABLE)\n",
    ")\n",
    "\n",
    "display(spark.table(BRONZE_TABLE).limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_smartds_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
